{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle as pkl\n",
    "from sklearn import datasets\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk import WordNetLemmatizer, word_tokenize, bigrams, ngrams, RegexpTokenizer\n",
    "import gensim\n",
    "from cycler import cycler\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "#cmap=plt.cm.tab10\n",
    "#c = cycler('color', cmap(np.linspace(0,1,10)))\n",
    "#plt.rcParams[\"axes.prop_cycle\"] = c\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "lemmatize = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = [lemmatize.lemmatize(token.lower().strip()) for token in tokens]\n",
    "    return ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('messages.pkl','rb') as f:\n",
    "    messages=pkl.load(f)\n",
    "    \n",
    "filename = 'text_files/inspiration_quotes.txt'\n",
    "with open(filename, \"r\") as file:\n",
    "    quotes = file.read()\n",
    "    \n",
    "quotes_clean = quotes.replace('\\n',' ')\n",
    "tokenizer = RegexpTokenizer(r'[“”]', gaps=True)\n",
    "quote_clean2 = [quote for quote in tokenizer.tokenize(quotes_clean) if quote[1] != '—']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do care'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "bad_list=['!','?',':','TANG','INA','PUTANG']\n",
    "def printify(s):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(list(filter(lambda x: (x in printable) and (x not in bad_list), s)))\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "d.check(\"Hello\")\n",
    "\n",
    "def anglify(m):\n",
    "    return ' '.join([x for x in word_tokenize(printify(m)) if d.check(x)])\n",
    "\n",
    "anglify(messages[4323])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_messages=[''.join(anglify(m)) for m in messages if len(m.split()) >50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(ngrams(word_tokenize('hi how are you doing'),3))\n",
    "\n",
    "def make_following(quote_clean2):\n",
    "    following = {}\n",
    "    count = {}\n",
    "    for quote in quote_clean2:\n",
    "        words = quote.split()\n",
    "        for index, word in enumerate(words[:-1]):\n",
    "            # ensure the word is in the dictionary\n",
    "            if word not in following:\n",
    "                following[word] = []\n",
    "                count[word] = 0\n",
    "\n",
    "            # append the next word\n",
    "     \n",
    "            following[word].append(words[index + 1])\n",
    "            count[word] += 1\n",
    "\n",
    "        if words:\n",
    "            last_word = words[-1]\n",
    "            if last_word not in following:\n",
    "                following[last_word] = []\n",
    "                count[last_word] = 0\n",
    "    return following\n",
    "\n",
    "def make_bigram(quote_clean2):\n",
    "    bigrams = {}\n",
    "    for quote in quote_clean2:\n",
    "        words = quote.split()\n",
    "        for w1, w2, w3 in zip(words[:-2], words[1:-1], words[2:]):\n",
    "            bigram = (w1,w2)\n",
    "            if bigram not in bigrams:\n",
    "                bigrams[bigram] = [w3]\n",
    "            else:\n",
    "                bigrams[bigram].append(w3)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def generate_text_bigram(following_dict, bigram_dict, num_words):\n",
    "    # randomly select a word.\n",
    "    # Ideas for improvements: \n",
    "    # - could mark words as sentence starters\n",
    "    # - could weight by frequency.\n",
    "    \n",
    "    all_bigrams = list(bigram_dict.keys())\n",
    "    current_bigram = random.choice(all_bigrams)\n",
    "    text = list(current_bigram)\n",
    "    followers=list(following_dict.keys())\n",
    "    \n",
    "    \n",
    "    \n",
    "    for _ in range(num_words - 1):\n",
    "        if current_bigram in all_bigrams:\n",
    "            next_word = random.choice(bigram_dict[current_bigram])\n",
    "        else:\n",
    "            \n",
    "            if len(text)==0:\n",
    "                next_word='of'\n",
    "            else:\n",
    "\n",
    "                next_word = random.choice(followers)\n",
    "\n",
    "        text.append(next_word)\n",
    "        current_bigram = (current_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n",
    "def cray_maker(text, num_words):\n",
    "    # randomly select a word.\n",
    "    # Ideas for improvements: \n",
    "    # - could mark words as sentence starters\n",
    "    # - could weight by frequency.\n",
    "    \n",
    "    bigram_dict=make_bigram(text)\n",
    "    following_dict=make_following(text)\n",
    "    \n",
    "    all_bigrams = list(bigram_dict.keys())\n",
    "    current_bigram = random.choice(all_bigrams)\n",
    "    text = list(current_bigram)\n",
    "    followers=list(following_dict.keys())\n",
    "    \n",
    "    \n",
    "    \n",
    "    for _ in range(num_words - 1):\n",
    "        if current_bigram in all_bigrams:\n",
    "            next_word = random.choice(bigram_dict[current_bigram])\n",
    "        else:\n",
    "            \n",
    "            if len(text)==0:\n",
    "                next_word='of'\n",
    "            else:\n",
    "\n",
    "                next_word = random.choice(followers)\n",
    "\n",
    "        text.append(next_word)\n",
    "        current_bigram = (current_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to pages I never even did shit . I your smile 2 . excited Karon next month including me ... .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cray_maker(long_messages,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will never be fulfilled. If your happiness depends on money, you'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'will never be fulfilled. If your happiness depends on money, you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'life’s big prizes. The Pulitzer. The Nobel. Oscars. The World Cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Life is a song, sing it. Life is a duty, complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'...and women - merely players. They have their exits and their entrances'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate some examples and save them for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_data=[]\n",
    "for _ in range(1000):\n",
    "    new=[cray_maker(long_messages,10),0]\n",
    "    markov_data.append(new)\n",
    "\n",
    "with open('markov_data.pkl','wb') as f:\n",
    "    pkl.dump(markov_data,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
